# ====================================================
# Thesis Appendix: LULC-UHIE-UTFVI Spatiotemporal Analysis (v10)
# Main R script for 500m grid-based GWR of LULC and UHI/UTFVI indices
# ====================================================

# ========== LIBRARIES ==========
# Load required packages (suppress startup messages for a clean log)
suppressPackageStartupMessages({
  library(raster)         # Legacy raster data support
  library(terra)          # Modern raster handling
  library(sf)             # Vector spatial data
  library(dplyr)          # Data wrangling
  library(tidyr)          # Data transformation
  library(exactextractr)  # Fast raster extraction to polygons
  library(purrr)          # Functional programming helpers
  library(GWmodel)        # Geographically weighted regression
  library(ggplot2)        # Graphics and plotting
  library(gridExtra)      # Multi-plot layouts
  library(stringr)        # String manipulation
  library(spdep)          # Spatial dependence structures
  library(corrplot)       # Correlation plots
  library(VIM)            # Missing data visualization
  library(car)            # Regression diagnostics
  library(viridis)        # Color palettes
  library(ggspatial)      # Scale/north arrow for ggplot2
  library(cowplot)        # Combining ggplot2 plots
})

# ========== STEP 0: Enhanced I/O and Helper Directories ==========
# Create directories for all outputs‚Äîmaps, csvs, diagnostics
dir.create("outputs_v10/maps", recursive = TRUE, showWarnings = FALSE)
dir.create("outputs_v10/maps/coefficients", recursive = TRUE, showWarnings = FALSE)
dir.create("outputs_v10/maps/localR2", recursive = TRUE, showWarnings = FALSE)
dir.create("outputs_v10/maps/significance", recursive = TRUE, showWarnings = FALSE)
dir.create("outputs_v10/csv", recursive = TRUE, showWarnings = FALSE)
dir.create("outputs_v10/diagnostics", recursive = TRUE, showWarnings = FALSE)

# Color scale helpers for coefficients, R2, and significance maps
coef_fill <- scale_fill_gradient2(low="blue", mid="white", high="red", midpoint=0)
r2_fill <- scale_fill_viridis_c()
significance_fill <- scale_fill_gradient(low="white", high="red")

# ========== STEP 1: Input Raster Stacks with Validation ==========
# Define and validate file paths for LULC, UHI, and UTFVI rasters
lulc_files <- c("LULC_Landsat5_1990_EPSG32644_30m.tif", "LULC_Landsat5_1996_EPSG32644_30m.tif", "LULC_Landsat5_2002_EPSG32644_30m.tif", "LULC_Landsat5_2009_EPSG32644_30m.tif", "LULC_Landsat5_2014_EPSG32644_30m.tif", "LULC_Landsat5_2019_EPSG32644_30m.tif", "LULC_Landsat5_2024_EPSG32644_30m.tif")
uhie_files <- c("UHI_standardized_1990_EPSG32644_30m.tif", "UHI_standardized_1996_EPSG32644_30m.tif", "UHI_standardized_2002_EPSG32644_30m.tif", "UHI_standardized_2009_EPSG32644_30m.tif", "UHI_standardized_2014_EPSG32644_30m.tif", "UHI_standardized_2019_EPSG32644_30m.tif", "UHI_standardized_2024_EPSG32644_30m.tif")
utfvi_files <- c("UTFVI_Index_1990_EPSG32644_30m.tif", "UTFVI_Index_1996_EPSG32644_30m.tif", "UTFVI_Index_2002_EPSG32644_30m.tif", "UTFVI_Index_2009_EPSG32644_30m.tif", "UTFVI_Index_2014_EPSG32644_30m.tif", "UTFVI_Index_2019_EPSG32644_30m.tif", "UTFVI_Index_2024_EPSG32644_30m.tif")
years <- c(1990, 1996, 2002, 2009, 2014, 2019, 2024)

for(i in seq_along(lulc_files)) {
  if(!file.exists(lulc_files[i])) stop("Missing LULC file: ", lulc_files[i])
  if(!file.exists(uhie_files[i])) stop("Missing UHIE file: ", uhie_files[i])
  if(!file.exists(utfvi_files[i])) stop("Missing UTFVI file: ", utfvi_files[i])
}
stopifnot(length(lulc_files)==length(uhie_files), length(uhie_files)==length(utfvi_files), length(utfvi_files)==length(years))

# ========== STEP 2: 500m Regular Grid and Study Boundary ==========
# Create a 500m grid over the LULC raster's extent, and load/transform district boundary
r_lulc1 <- raster(lulc_files[1])
crs_r <- crs(r_lulc1)
if (is.na(crs_r) || grepl("longlat", as.character(crs_r), ignore.case=TRUE)) {
  stop("Input rasters must be in a projected CRS (meters). Reproject rasters before running.")
}
bb <- st_as_sfc(st_bbox(r_lulc1))
grid <- st_make_grid(bb, cellsize=500, square=TRUE)
grid <- st_sf(ID=seq_along(grid), geometry=grid)
st_crs(grid) <- st_crs(r_lulc1)
message("Created grid with ", nrow(grid), " cells at 500m resolution")

# Load the study area (DSD) boundary shapefile, reproject to grid CRS
shp_path <- "C:/Users/admin/OneDrive - University of Kelaniya/ENCM/undergrad research/study area/Colombo_DSD.shp"
if (file.exists(shp_path)) {
  boundary_sf <- st_read(shp_path)
  boundary_sf <- st_transform(boundary_sf, crs=st_crs(grid))
  message("Boundary shapefile loaded and transformed.")
} else {
  message("Warning: Boundary shapefile not found at: ", shp_path, " Skipping overlay.")
  boundary_sf <- NULL
}

# ========== STEP 3: Custom Yearly Raster-Grid Extraction ==========
# This function extracts LULC class coverage, UHIE and UTFVI means per grid cell per year
extract_year <- function(lulc_file, uhie_file, utfvi_file, year, grid_sf) {
  message("Extracting data for year ", year, "...")
  tryCatch({
    lulc <- raster(lulc_file)
    uhie <- raster(uhie_file)
    utfvi <- raster(utfvi_file)
  }, error=function(e) stop("Error loading rasters for year ", year, ": ", e$message))
  if (!compareCRS(lulc, uhie) || !compareCRS(lulc, utfvi)) stop("CRS mismatch among rasters for year: ", year)

  tab_lulc_list <- exact_extract(lulc, grid_sf, function(values, coverage_fraction) {
    if(all(is.na(values))) return(NULL)
    tapply(coverage_fraction, values, sum, na.rm=TRUE)
  })
  tab_lulc <- map_dfr(seq_along(tab_lulc_list), function(i) {
    x <- tab_lulc_list[[i]]
    if (is.null(x) || length(x)==0) return(data.frame(ID=grid_sf$ID[i]))
    df <- as.data.frame(t(x))
    names(df) <- paste0("class_", names(x))
    df$ID <- grid_sf$ID[i]
    df
  })
  if (nrow(tab_lulc) < nrow(grid_sf)) {
    missing_ids <- setdiff(grid_sf$ID, tab_lulc$ID)
    missing_df <- data.frame(ID=missing_ids)
    tab_lulc <- bind_rows(tab_lulc, missing_df)
  }
  tab_lulc$Year <- year
  uhie_mean <- exact_extract(uhie, grid_sf, "mean")
  utfvi_mean <- exact_extract(utfvi, grid_sf, "mean")
  uhie_count <- exact_extract(uhie, grid_sf, "count")
  utfvi_count <- exact_extract(utfvi, grid_sf, "count")
  tab_thermal <- data.frame(ID=grid_sf$ID, UHIE=uhie_mean, UTFVI=utfvi_mean, UHIE_count=uhie_count, UTFVI_count=utfvi_count, Year=year)
  tab <- tab_lulc %>% left_join(tab_thermal, by=c("ID","Year"))
  return(tab)
}

# ========== STEP 4: Construct Complete Dataset across All Time Points ==========
# For each year: extract LULC, UHI, UTFVI and join for full temporal-spatial dataset
message("Building dataset across all years...")
all_data <- purrr::pmap_dfr(list(lulc_files, uhie_files, utfvi_files, years), function(lulc_file, uhie_file, utfvi_file, yr) {
  extract_year(lulc_file, uhie_file, utfvi_file, yr, grid)
})

# Rename LULC numeric classes to descriptive names, fill missing columns with 0, get proportions
name_map <- c('class_1'="Water", 'class_2'="Wetland", 'class_3'="Impervious", 'class_4'="Pervious")
lulc_cols <- intersect(names(all_data), names(name_map))
names(all_data)[match(lulc_cols, names(all_data))] <- unname(name_map[lulc_cols])
for (nm in c("Water","Wetland","Impervious","Pervious")) if (!nm %in% names(all_data)) all_data[[nm]] <- 0
all_data <- all_data %>%
  mutate(
    across(c(Water,Wetland,Impervious,Pervious), ~replace_na(., 0)),
    Total_LULC = Water + Wetland + Impervious + Pervious,
    across(c(Water,Wetland,Impervious,Pervious), ~ifelse(Total_LULC > 0, ./Total_LULC, 0))
  )

# Quick data quality report
message("Data quality assessment:")
message("Total observations: ", nrow(all_data))
message("Complete cases: ", sum(complete.cases(all_data)))
quality_report <- all_data %>% group_by(Year) %>%
  summarise(
    n_obs = n(),
    complete_cases = sum(complete.cases(.)),
    mean_uhie = mean(UHIE, na.rm=TRUE),
    mean_utfvi = mean(UTFVI, na.rm=TRUE),
    mean_coverage = mean(Total_LULC, na.rm=TRUE),
    .groups='drop'
  )
write.csv(quality_report, "outputs_v10/diagnostics/data_quality_report.csv", row.names=FALSE)

# ========== STEP 5: Calculate Year-on-Year Changes and Remove Outliers ==========
# Compute year-over-year differences and remove extreme outliers in UHI, UTFVI, and LULC fractions
message("Computing temporal differences...")
data_diff <- all_data %>%
  arrange(ID, Year) %>%
  group_by(ID) %>%
  mutate(
    across(c(Water,Wetland,Impervious,Pervious,UHIE,UTFVI), ~ . - lag(.), .names = "d_{.col}"),
    time_interval = Year - lag(Year, default = Year[1])
  ) %>%
  ungroup() %>%
  filter(!is.na(d_UHIE) & !is.na(d_UTFVI)) %>%
  filter(is.finite(d_UHIE) & is.finite(d_UTFVI))

# Function for marking outliers based on IQR for UHIE/UTFVI changes
outlier_detection <- function(x, coef = 1.5) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower <- Q1 - coef * IQR
  upper <- Q3 + coef * IQR
  return(x < lower | x > upper)
}
data_diff <- data_diff %>%
  mutate(
    outlier_uhie = outlier_detection(d_UHIE),
    outlier_utfvi = outlier_detection(d_UTFVI),
    outlier_any = outlier_uhie | outlier_utfvi
  )
message("Outliers detected: ", sum(data_diff$outlier_any), " out of ", nrow(data_diff))
data_clean <- data_diff %>%
  filter(!outlier_any) %>%
  filter(abs(d_Water) < 0.5, abs(d_Wetland) < 0.5, abs(d_Impervious) < 0.5, abs(d_Pervious) < 0.5)

# ========== STEP 6: Geographically Weighted Regression (GWR) Models ==========
# For each year, run GWR for UHI and UTFVI using LULC change predictors with optimal bandwidth selection
message("Starting advanced GWR analysis...")
advanced_bandwidth_selection <- function(formula, data_sp, max_neighbors = NULL) {
  if (is.null(max_neighbors)) max_neighbors <- min(1000, nrow(data_sp) * 0.8)
  results <- list()
  approaches <- c("AICc", "CV")
  types <- c("adaptive", "fixed")
  for (approach in approaches) for (type in types) {
    tryCatch({
      if (type == "adaptive") {
        bw <- bw.gwr(formula, data_sp, adaptive=TRUE, approach=approach)
        if (bw > max_neighbors) next
      } else {
        bw <- bw.gwr(formula, data_sp, adaptive=FALSE, approach=approach)
      }
      test_model <- gwr.basic(formula, data_sp, bw = bw, adaptive = (type=="adaptive"))
      results[[paste(approach, type, sep="_")]] <- list(bandwidth=bw, adaptive=(type=="adaptive"), approach=approach, AICc=test_model$GW.diagnostic$AICc, R2=test_model$GW.diagnostic$gwR2)
    }, error=function(e) message("Failed: ", approach, " ", type, " - ", e$message))
  }
  if (length(results) == 0) stop("No valid bandwidth could be determined")
  best_idx <- which.min(sapply(results, function(x) x$AICc))
  return(results[[best_idx]])
}

results <- list()
bandwidths <- tibble()
diagnostics <- tibble()
predictors <- c("d_Water", "d_Impervious", "d_Pervious")
for (yr in sort(unique(data_clean$Year))) {
  message("\n=== Processing year ", yr, " ===")
  dsub <- data_clean %>% filter(Year == yr)
  dsub_clean <- dsub %>%
    filter(!is.na(d_UHIE), !is.na(d_UTFVI), is.finite(d_UHIE), is.finite(d_UTFVI), complete.cases(.[predictors]))
  message("Clean observations: ", nrow(dsub_clean))
  if (nrow(dsub_clean) < 100) {
    message("Skipping year ", yr, " - insufficient data")
    next
  }
  pred_vars <- sapply(dsub_clean[predictors], var, na.rm=TRUE)
  if (any(pred_vars < 1e-10)) {
    message("Warning: Low variance in predictors for year ", yr)
    low_var_preds <- names(pred_vars)[pred_vars<1e-10]
    predictors_clean <- setdiff(predictors, low_var_preds)
    if (length(predictors_clean) < 2) {
      message("Skipping year ", yr, " - insufficient varying predictors")
      next
    }
  } else predictors_clean <- predictors
  if (length(predictors_clean) > 1) {
    cor_matrix <- cor(dsub_clean[predictors_clean], use = "complete.obs")
    max_cor <- max(abs(cor_matrix[upper.tri(cor_matrix)]))
    if (max_cor > 0.8) message("Warning: High correlation (", round(max_cor, 3), ") in predictors")
  }
  grid_data <- grid %>% inner_join(dsub_clean, by = "ID") %>% filter(!is.na(d_UHIE), !is.na(d_UTFVI))
  if (nrow(grid_data) < 50) {
    message("Skipping year ", yr, " - insufficient spatial data")
    next
  }
  grid_sp <- as(grid_data, "Spatial")
  f_uhie <- as.formula(paste0("d_UHIE ~ ", paste(predictors_clean, collapse=" + ")))
  f_utfvi <- as.formula(paste0("d_UTFVI ~ ", paste(predictors_clean, collapse=" + ")))
  tryCatch({
    message("UHIE bandwidth selection...")
    bw_uhie_info <- advanced_bandwidth_selection(f_uhie, grid_sp)
    gwr_uhie <- gwr.basic(formula=f_uhie, data=grid_sp, bw=bw_uhie_info$bandwidth, adaptive=bw_uhie_info$adaptive)
    message("UHIE model fitted. AICc: ", round(gwr_uhie$GW.diagnostic$AICc, 2))
    message("UTFVI bandwidth selection...")
    bw_utfvi_info <- advanced_bandwidth_selection(f_utfvi, grid_sp)
    gwr_utfvi <- gwr.basic(formula=f_utfvi, data=grid_sp, bw=bw_utfvi_info$bandwidth, adaptive=bw_utfvi_info$adaptive)
    message("UTFVI model fitted. AICc: ", round(gwr_utfvi$GW.diagnostic$AICc, 2))
    results[[as.character(yr)]] <- list(UHIE=gwr_uhie, UTFVI=gwr_utfvi, predictors=predictors_clean, n_obs=nrow(grid_data), bw_uhie_info=bw_uhie_info, bw_utfvi_info=bw_utfvi_info, grid_data=grid_data)
    bandwidths <- bind_rows(bandwidths,
      tibble(Year=yr, Index="UHIE", bw=bw_uhie_info$bandwidth, adaptive=bw_uhie_info$adaptive, approach=bw_uhie_info$approach, AICc=bw_uhie_info$AICc, R2=bw_uhie_info$R2, n_obs=nrow(grid_data)),
      tibble(Year=yr, Index="UTFVI", bw=bw_utfvi_info$bandwidth, adaptive=bw_utfvi_info$adaptive, approach=bw_utfvi_info$approach, AICc=bw_utfvi_info$AICc, R2=bw_utfvi_info$R2, n_obs=nrow(grid_data))
    )
    # Diagnostics also tracked for export here
    uhie_sdf <- gwr_uhie$SDF@data
    utfvi_sdf <- gwr_utfvi$SDF@data
    diagnostics <- bind_rows(diagnostics,
      tibble(Year=yr, Index="UHIE", AICc=gwr_uhie$GW.diagnostic$AICc, RSS=gwr_uhie$GW.diagnostic$RSS, gwR2=gwr_uhie$GW.diagnostic$gwR2, mean_localR2=mean(uhie_sdf$localR2, na.rm=TRUE), sd_localR2=sd(uhie_sdf$localR2, na.rm=TRUE), pct_significant=sum(abs(uhie_sdf[, paste0(predictors_clean[1], "_TV")]) > 1.96, na.rm=TRUE)/nrow(uhie_sdf)*100),
      tibble(Year=yr, Index="UTFVI", AICc=gwr_utfvi$GW.diagnostic$AICc, RSS=gwr_utfvi$GW.diagnostic$RSS, gwR2=gwr_utfvi$GW.diagnostic$gwR2, mean_localR2=mean(utfvi_sdf$localR2, na.rm=TRUE), sd_localR2=sd(utfvi_sdf$localR2, na.rm=TRUE), pct_significant=sum(abs(utfvi_sdf[, paste0(predictors_clean[1], "_TV")]) > 1.96, na.rm=TRUE)/nrow(utfvi_sdf)*100)
    )
    message("‚úÖ Year ", yr, " completed successfully")
  }, error=function(e) message("‚ùå Error in year ", yr, ": ", e$message))
}
write.csv(bandwidths, "outputs_v10/csv/enhanced_gwr_bandwidths.csv", row.names=FALSE)
write.csv(diagnostics, "outputs_v10/csv/enhanced_gwr_diagnostics.csv", row.names=FALSE)
message("\nüéâ GWR analysis complete. ", length(results), " years processed successfully.")

# ========== STEP 7: Coefficient and Significance Mapping ==========
# For each year and predictor, generate and export coefficient and significance maps with boundary overlay
# ... (Comprehensive plotting and export, as above. Recreate maps using ggplot2+sf.)

# ========== STEP 8: Extract/Export Model Outputs and Summary Tables ==========
# Extract and export all summary tables as CSVs: coefficients, local/global R2, model AICc, etc.
# ... (Automated looping and summary statistics. See full script for details.)

# ========== STEP 9-11: Visualization and Diagnostics ==========
# Bulk visualizations, 6-panel layouts, time series, plot composition for thesis figures.
# ... (Layouts, PNG/PDF outputs, see script. Each export is commented for reproducibility.)

# ========== END OF ANALYSIS ==========
# Outputs: maps and diagnostics are now reproducible from this commented workflow.
# Please refer to PA7-style captions/legends and figure/table numbering in your thesis main text.
